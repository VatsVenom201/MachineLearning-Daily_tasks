{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff4fd470-70f7-44e3-a980-c7dd1277226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b333686-50d7-443b-bc85-f51e7dbcb953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.txt', sep=';', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08effad-2688-43bd-abbe-e51663d60581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                para      emo\n",
       "0                            i didnt feel humiliated  sadness\n",
       "1  i can go from feeling so hopeless to so damned...  sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong    anger\n",
       "3  i am ever feeling nostalgic about the fireplac...     love\n",
       "4                               i am feeling grouchy    anger"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={\n",
    "    0: 'para',\n",
    "    1: 'emo'\n",
    "}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c3f55b-3d81-4ce2-98f5-d89ecf75ee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "para    0\n",
       "emo     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af599529-2622-424b-887a-7153630fe805",
   "metadata": {},
   "source": [
    "## Manuall Encoding of Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9202e5c3-3f02-4a9d-9769-cc686f58c973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_emo = df['emo'].unique()\n",
    "unique_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2023f930-1751-4321-94ab-d9f211578d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_label = {\n",
    "    'sadness':1,\n",
    "    'anger':2,\n",
    "    'love':3,\n",
    "    'surprise':4,\n",
    "    'fear':5,\n",
    "    'joy':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76daaddd-4013-46af-876b-b66ea9d95757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                para  emo\n",
       "0                            i didnt feel humiliated    1\n",
       "1  i can go from feeling so hopeless to so damned...    1\n",
       "2   im grabbing a minute to post i feel greedy wrong    2\n",
       "3  i am ever feeling nostalgic about the fireplac...    3\n",
       "4                               i am feeling grouchy    2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emo'] = df['emo'].map(emo_label)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab11776-d118-43e4-968f-bb9c8179b0ff",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d70f2d-285a-4023-afba-b6d256ab410a",
   "metadata": {},
   "source": [
    "### Converting to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42999c4-0d55-44ea-bfa1-4ed130611703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['para'] = df['para'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb007d7-3a38-4152-bd2c-1d34b8d9a02b",
   "metadata": {},
   "source": [
    "### Removing Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004aa6d0-6d0f-4c9b-8c35-9f67d296a6c8",
   "metadata": {},
   "source": [
    "#### 1) using string functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4dd1ba-2ec9-4844-9b4a-002e17ab3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8435cbd4-539f-4681-971d-fe8bc06aeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(txt):\n",
    "    return txt.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a28e3abb-43f9-493b-82c8-00d884b5c80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                para  emo\n",
       "0                            i didnt feel humiliated    1\n",
       "1  i can go from feeling so hopeless to so damned...    1\n",
       "2   im grabbing a minute to post i feel greedy wrong    2\n",
       "3  i am ever feeling nostalgic about the fireplac...    3\n",
       "4                               i am feeling grouchy    2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['para'] = df['para'].apply(remove_punc)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f03ea6-5882-49b8-b8c0-185e0d2e0815",
   "metadata": {},
   "source": [
    "#### 2) removing punctuations using Regular Exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a03e58-511e-4216-94da-b1f50ca3ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2e85f02-0621-4a99-8b37-cf7f4dd7e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pun(txt):\n",
    "    \n",
    "    txt = rg.sub(r'[^a-zA-Z\\s]','',txt)   # (string/pattern to be replace, replace with , text )\n",
    "    # ^ : excluding these...\n",
    "    # a-z : small letters\n",
    "    # A-Z : large letters\n",
    "    # \\s : whitespace\n",
    "    return txt\n",
    "    # so this r'[^a-zA-Z\\s]' includes punctuations,numbers...we have ^ as exclusion mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5522dd-a025-45a9-a310-8e133cfae25e",
   "metadata": {},
   "source": [
    "This regex removes all of the following:\n",
    "\n",
    "Punctuation: ! , . ?\n",
    "\n",
    "Digits: 0â€“9\n",
    "\n",
    "Emojis: ðŸ™‚ ðŸ”¥ ðŸ’”\n",
    "\n",
    "Symbols: â‚¹ â‚¬ Â©\n",
    "\n",
    "Accented letters: Ã© Ã± Ã¼\n",
    "\n",
    "Non-English scripts: à¤¹à¤¿à¤‚à¤¦à¥€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b957766-5082-43dc-8edb-25847897903a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  i didnt feel humiliated\n",
       "1        i can go from feeling so hopeless to so damned...\n",
       "2         im grabbing a minute to post i feel greedy wrong\n",
       "3        i am ever feeling nostalgic about the fireplac...\n",
       "4                                     i am feeling grouchy\n",
       "                               ...                        \n",
       "15995    i just had a very brief time in the beanbag an...\n",
       "15996    i am now turning and i feel pathetic that i am...\n",
       "15997                       i feel strong and good overall\n",
       "15998    i feel like this was such a rude comment and i...\n",
       "15999    i know a lot but i feel so stupid because i ca...\n",
       "Name: para, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['para'] = df['para'].apply(remove_pun)\n",
    "df['para']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53ecbc7f-b7f8-4539-b07f-24e4528c9f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                para  emo\n",
       "0                            i didnt feel humiliated    1\n",
       "1  i can go from feeling so hopeless to so damned...    1\n",
       "2   im grabbing a minute to post i feel greedy wrong    2\n",
       "3  i am ever feeling nostalgic about the fireplac...    3\n",
       "4                               i am feeling grouchy    2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979776ed-554f-4558-b0d4-070ea50acdba",
   "metadata": {},
   "source": [
    "### Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddbcf330-443e-4592-8c94-e1e067702cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a manual function\n",
    "\n",
    "def remove_emoji(txt):\n",
    "    new=''\n",
    "    for i in txt:\n",
    "        if i.isascii(): # emoji's aren't in ascii range, 0-127 it inlcudes small/capital letters and 0-9 only\n",
    "            new += i\n",
    "    return new     \n",
    "\n",
    "# similarly we can remove numbers with i.isdigit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eddf74e-afa3-469f-a774-02629cb1d841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  i didnt feel humiliated\n",
       "1        i can go from feeling so hopeless to so damned...\n",
       "2         im grabbing a minute to post i feel greedy wrong\n",
       "3        i am ever feeling nostalgic about the fireplac...\n",
       "4                                     i am feeling grouchy\n",
       "                               ...                        \n",
       "15995    i just had a very brief time in the beanbag an...\n",
       "15996    i am now turning and i feel pathetic that i am...\n",
       "15997                       i feel strong and good overall\n",
       "15998    i feel like this was such a rude comment and i...\n",
       "15999    i know a lot but i feel so stupid because i ca...\n",
       "Name: para, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['para'] = df['para'].apply(remove_emoji)\n",
    "df['para']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada0510-5750-4fd6-90a8-c18e70b00e6c",
   "metadata": {},
   "source": [
    "### Removing extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f541c457-9645-42c8-84a8-d0ce39a365e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_whitespace(txt):\n",
    "    return rg.sub(r'\\s+', ' ', txt).strip()\n",
    "\n",
    "# \\s : includes whitespace, [ \\t\\n\\r\\f\\v]\n",
    "# \\s+ : matches one or more than 1 whitespace charc, thoise charc are [ \\t\\n\\r\\f\\v]\n",
    "# .strip removes whitespaces in beginning and end of text...not what s in middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eea8c7d0-e294-407e-871f-d4be4ea5386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['para'] = df['para'].apply(clean_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2313a44b-42fa-46c5-b50b-94e08bb8ddbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  i didnt feel humiliated\n",
       "1        i can go from feeling so hopeless to so damned...\n",
       "2         im grabbing a minute to post i feel greedy wrong\n",
       "3        i am ever feeling nostalgic about the fireplac...\n",
       "4                                     i am feeling grouchy\n",
       "                               ...                        \n",
       "15995    i just had a very brief time in the beanbag an...\n",
       "15996    i am now turning and i feel pathetic that i am...\n",
       "15997                       i feel strong and good overall\n",
       "15998    i feel like this was such a rude comment and i...\n",
       "15999    i know a lot but i feel so stupid because i ca...\n",
       "Name: para, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['para']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7d51e-c853-4345-8bb1-31063fdb98bd",
   "metadata": {},
   "source": [
    "### Removing HTML tags, for webscrapped data\n",
    "\n",
    "we use reg.exp. r'<[^>]+>'\n",
    "\n",
    "< is literal 1st tag..\n",
    "\n",
    "[ ] is a character class, where ^ says exclude this, > is literal exclusion of >\n",
    "\n",
    "so [^>] is Match any character that is NOT >\n",
    "\n",
    "'+' is one or more charc ,Keep matching [^>] as long as possible\n",
    "\n",
    "Final > : Matches the literal >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01902c-dc4a-4318-bd3a-6a773dea64a4",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "'is' 'the' 'are'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47f35f-086f-4753-8e4a-a66a258f2d17",
   "metadata": {},
   "source": [
    "### we use NLTK - natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79ce64e5-ec42-432c-ae15-9d8e59e9d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db6d684b-79d7-454e-b096-21eed1a60260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "139f0b9b-4108-4c3e-8998-119f75a04ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vatsc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vatsc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vatsc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have to download stopwords and tokenization resources\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    " # Punkt is a pre-trained sentence tokenizer\n",
    "nltk.download('stopwords')\n",
    " # stopwords of all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2db7336e-7d89-4cc1-ac32-b13dc9c2c5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cddc05e-0e78-41ce-9a23-176f8c56382e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no. of stopwords of english..\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c5f10-da84-4e2e-b921-69d42a2e0246",
   "metadata": {},
   "source": [
    "### Tokenizing & Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d74e9425-a6d9-4ba9-ba17-dcf100fb3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(txt):\n",
    "    words = word_tokenize(txt)\n",
    "    new = []\n",
    "    for i in words:\n",
    "      if i not in stop_words: # we have to check membership of i with list of stop_words...not equality !=\n",
    "          new.append(i) # new += i will return each letter seperatly..good -> g,o,o,d\n",
    "    return ' '.join(new) \n",
    "\n",
    "    # join() converts a list of strings into ONE string\n",
    "    # separator.join(iterable_of_strings)\n",
    "    # return new will just return a list, not string we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dafe4f73-251f-4207-83ef-55afb552b5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    didnt feel humiliated\n",
       "1        go feeling hopeless damned hopeful around some...\n",
       "2                im grabbing minute post feel greedy wrong\n",
       "3        ever feeling nostalgic fireplace know still pr...\n",
       "4                                          feeling grouchy\n",
       "                               ...                        \n",
       "15995        brief time beanbag said anna feel like beaten\n",
       "15996    turning feel pathetic still waiting tables sub...\n",
       "15997                             feel strong good overall\n",
       "15998                       feel like rude comment im glad\n",
       "15999                         know lot feel stupid portray\n",
       "Name: para, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['para'] = df['para'].apply(remove_stop)\n",
    "df['para']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c8c2a-b261-403b-b294-de42e7dec4fc",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "Bag of Words is a text vectorization technique that converts text documents into numerical vectors based on word frequency.\n",
    "\n",
    "How it works\n",
    "\n",
    "Build a vocabulary of all unique words in the corpus\n",
    "\n",
    "For each document:\n",
    "\n",
    "Count how many times each word appears\n",
    "\n",
    "Order does not matter\n",
    "\n",
    "Example\n",
    "\n",
    "Documents:\n",
    "\n",
    "\"I love ML\"\n",
    "\n",
    "\"I love AI\"\n",
    "\n",
    "Vocabulary:\n",
    "[I, love, ML, AI]\n",
    "\n",
    "Vectors:\n",
    "| I | love | ML | AI |\n",
    "| - | ---- | -- | -- |\n",
    "| 1 | 1    | 1  | 0  |\n",
    "| 1 | 1    | 0  | 1  |\n",
    "\n",
    "\n",
    "tool - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2918bef5-a4ab-415c-8521-c5a8a46d9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646e36e-a1f8-4643-b954-5dee4380f4bb",
   "metadata": {},
   "source": [
    "#### we have not yet done stemming so all words gets a unique col, all words gets included in vocabulary of BagofWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd5e5235-2ea2-4163-b073-87f9171bd2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['aa' 'aaaaaaand' 'aaaaand' ... 'zum' 'zumba' 'zz']\n",
      "\n",
      "Matrix: \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['para'])\n",
    "print('Vocabulary: ',vectorizer.get_feature_names_out())\n",
    "print('\\nMatrix: \\n',X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60590c-ec2c-42bd-89c3-2f6df2373ccb",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Similar words like 'accept', 'accepted', 'accepts'...same meaning with diff words\n",
    "\n",
    "like in above vectorizer we got 'aa' 'aaaaaaand' 'aaaaand seperatly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46ce455f-8cfe-4b06-b322-8050267081c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a27ceea5-3c32-4a29-ac20-15f5e47936da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(txt):\n",
    "    words = word_tokenize(txt)\n",
    "    new = []\n",
    "    for i in words:\n",
    "        i = ps.stem(i)\n",
    "        new.append(i)\n",
    "    return ' '.join(new)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdebc2b9-d2ee-4f2d-9422-a9e042d56f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        didnt feel humili\n",
       "1        go feel hopeless damn hope around someon care ...\n",
       "2                     im grab minut post feel greedi wrong\n",
       "3           ever feel nostalg fireplac know still properti\n",
       "4                                             feel grouchi\n",
       "                               ...                        \n",
       "15995        brief time beanbag said anna feel like beaten\n",
       "15996     turn feel pathet still wait tabl sub teach degre\n",
       "15997                              feel strong good overal\n",
       "15998                       feel like rude comment im glad\n",
       "15999                         know lot feel stupid portray\n",
       "Name: para, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['para'] = df['para'].apply(stemming)\n",
    "df['para']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b76cd-4bc8-4156-be6e-a0c46444d97f",
   "metadata": {},
   "source": [
    "#### now we do vectorization again after stemming..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "285832e7-acfa-4348-b904-f582a8b2b83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['aa' 'aaaaaaand' 'aaaaand' 'aaaand' 'aac' 'aahhh' 'aaron' 'ab' 'abandon'\n",
      " 'abat' 'abbigail' 'abc' 'abdomen' 'abdomin' 'abduct' 'abelard' 'abhorr'\n",
      " 'abid' 'abil' 'abit' 'abl' 'abnorm' 'aboard' 'abomin' 'abort' 'abou'\n",
      " 'abound' 'abraham' 'abroad' 'abruptli' 'absenc' 'absolut' 'absolutli'\n",
      " 'absorb' 'abstain' 'abstin' 'abstract' 'absurd' 'abt' 'abund'\n",
      " 'abundantli' 'abus' 'abyss' 'ac' 'academ' 'academi' 'academia' 'acaus'\n",
      " 'acceler' 'accent']\n",
      "\n",
      "Matrix: \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['para'])\n",
    "print('Vocabulary: ',vectorizer.get_feature_names_out()[0:50])\n",
    "print('\\nMatrix: \\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "251f008d-e1e1-448c-80b6-37e6fdbec957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 10357)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9987a79-f3c0-4bc1-8c29-6a7cbde7b39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10357,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fae890-1a8c-4567-9ac8-1c52106927cc",
   "metadata": {},
   "source": [
    "#### n-gram vectorization...\n",
    "in short - instead of bagging each word 1 by 1, we do bagging of joined words\n",
    "\n",
    "eg-\n",
    "\n",
    "insta is good\n",
    "\n",
    "ngram_range(1,1)\n",
    "\n",
    "['insta' , 'is' , 'good']\n",
    "\n",
    "ngram_range(1,2)\n",
    "\n",
    "['insta is' , 'is good']\n",
    "\n",
    "Docstring:\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "\n",
    "benifit is that we get more semantic value of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce710e8-3c74-42a4-8e4b-cc5194ad0e7f",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizer\n",
    "\n",
    "A TF-IDF score tells you how important a word is to a document, relative to the entire corpus.\n",
    "\n",
    "â€œIs this word characteristic of this document, or is it just common everywhere?â€\n",
    "\n",
    "TF = Term Frequncy (how often word occurs in doc)\n",
    "\n",
    "TF(t,d)= count of term t in document d / total words in document d\n",
    "\n",
    "IDF = Inverse Doc Freq. (rarity of word)\n",
    "\n",
    "IDF(t,d) = log base e ( total no. of doc in corpus /  no. of docs having t )\n",
    "\n",
    "TF*IDF = tfidf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0568e71-be57-4a48-9e03-c445f33c95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d05963c6-bf1d-4c04-8f9f-9d97cb6268e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['para'])\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2716281-b9d3-4e65-9cf3-e9ed101b641b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaaaaaand', 'aaaaand', 'aaaand', 'aac', 'aahhh', 'aaron',\n",
       "       'ab', 'abandon', 'abat', 'abbigail', 'abc', 'abdomen', 'abdomin',\n",
       "       'abduct', 'abelard', 'abhorr', 'abid', 'abil', 'abit', 'abl',\n",
       "       'abnorm', 'aboard', 'abomin', 'abort', 'abou', 'abound', 'abraham',\n",
       "       'abroad', 'abruptli', 'absenc', 'absolut', 'absolutli', 'absorb',\n",
       "       'abstain', 'abstin', 'abstract', 'absurd', 'abt', 'abund',\n",
       "       'abundantli', 'abus', 'abyss', 'ac', 'academ', 'academi',\n",
       "       'academia', 'acaus', 'acceler', 'accent'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "096547d6-69e4-43a3-86a8-49f9e70ca5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_count = cv.fit_transform(df['para'])\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['para'])\n",
    "\n",
    "print(X_count[0].toarray())\n",
    "print(X_tfidf[0].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12bc2f4a-fea1-433e-85b9-72d2f9f970f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "      <th>emo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>didnt feel humili</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go feel hopeless damn hope around someon care ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grab minut post feel greedi wrong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ever feel nostalg fireplac know still properti</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel grouchi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                para  emo\n",
       "0                                  didnt feel humili    1\n",
       "1  go feel hopeless damn hope around someon care ...    1\n",
       "2               im grab minut post feel greedi wrong    2\n",
       "3     ever feel nostalg fireplac know still properti    3\n",
       "4                                       feel grouchi    2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6687124-09c4-4e51-bb8a-0e006ca3a775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].nnz\n",
    "# non zero in 1st doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c87a4f55-477c-4a3a-9673-79d6d2c899f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>didnt</th>\n",
       "      <th>feel</th>\n",
       "      <th>humili</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600113</td>\n",
       "      <td>0.118816</td>\n",
       "      <td>0.791041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      didnt      feel    humili\n",
       "0  0.600113  0.118816  0.791041"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying 1st doc where its nonzero\n",
    "pd.DataFrame(\n",
    "    X[0].toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ").loc[:, lambda df: df.iloc[0] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcf246-b277-4838-bb7c-5d782f3d7bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
